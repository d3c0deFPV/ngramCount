{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 60em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make output bigger for jupyter notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 60em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curl response too big for display, 117333 bytes, max is 50000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pycurl\n",
    "import certifi\n",
    "import re\n",
    "from io import BytesIO \n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "#Options\n",
    "dataURL = 'https://www.reddit.com/r/space/.json?count=250'\n",
    "killCase =            1      #Make everything lowercase\n",
    "parse =               1      #Basic parsing using parseRegex variable\n",
    "replacebadRegex =     0      #Replace pattern in badRegex\n",
    "replacebadChars =     1      #Replace characters in badChars\n",
    "onlybasicChars =      0      #Only alphanumeric and spaces\n",
    "splitbychar =         0      #Split by character (default is space)\n",
    "ngramMin =            2      #Smallest ngram to look for\n",
    "ngramMax =            20     #Largest ngram to look for\n",
    "minngramCount =       2      #Minimum number of times an ngram must be repeated\n",
    "ngramRedundant =      1      #Show non-unqiue ngrams inside of other larger ones\n",
    "mostcommonMax =     100      #How many ngrams max to return from most_common()\n",
    "responsesizeMax = 50000      #Largest response size to show\n",
    "ignoreSSLwarn =       0      #Don't check SSL certificates for curl\n",
    "showresponses =       1      #Show curl response and data\n",
    "\n",
    "\n",
    "#Regex matches\n",
    "badChars = [';', ':', '!', \"*\", \"<\", \">\", \"-\", \"\\\"\", \"=\"] \n",
    "badRegex = re.compile(r\"<[^>]*>\")\n",
    "parseRegex = re.compile(r'\"title\": \"(.*?)\"', flags=re.I|re.M) #reddit titles\n",
    "#badRegex = re.compile(r'wiki')\n",
    "#badRegex = re.compile(r\".*?<body>(.*?)</body>\")\n",
    "alphanumspaceRegex = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
    "\n",
    "#Curl page for data\n",
    "b_obj = BytesIO() \n",
    "crl = pycurl.Curl() \n",
    "crl.setopt(crl.URL, dataURL)\n",
    "if ignoreSSLwarn == 1:\n",
    "    crl.setopt(pycurl.SSL_VERIFYPEER, 0) #trust invalid SSL\n",
    "    crl.setopt(pycurl.SSL_VERIFYHOST, 0) #trust invalid SSL\n",
    "crl.setopt(crl.WRITEDATA, b_obj)\n",
    "crl.setopt(pycurl.USERAGENT, \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\")\n",
    "crl.setopt(pycurl.CAINFO, certifi.where())\n",
    "crl.perform() \n",
    "crl.close()\n",
    "get_body = b_obj.getvalue()\n",
    "curlResponse = str(get_body.decode('utf8'))\n",
    "\n",
    "#Print responses if enabled\n",
    "if showresponses == 1:\n",
    "    curlResponseSize = sys.getsizeof(curlResponse)\n",
    "    #Don't print huge amounts of data\n",
    "    if (curlResponseSize < responsesizeMax):\n",
    "        print(curlResponse)\n",
    "    else:\n",
    "        print(\"Curl response too big for display, \" + str(curlResponseSize) + \" bytes, max is \" + str(responsesizeMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching regex: \"title\": \"(.*?)\"\n",
      "Before size:\t114.58\tKB\n",
      "After size:\t2.76\tKB\n",
      "\n",
      "Removing the following chars: [';', ':', '!', '*', '<', '>', '-', '\"', '=']\n",
      "Before size:\t2.76\tKB\n",
      "After size:\t2.75\tKB\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Common ngrams, longest to shortest: \n",
      "\n",
      "3\tof space\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ngram search time:\t 0.011010885238647461 seconds\n",
      "Total processing time:\t 0.014000177383422852 seconds\n",
      "\n",
      "Source:\thttps://www.reddit.com/r/space/.json?count=250\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "showresponses = 1, displaying data...\n",
      "combinedngram:\n",
      "\n",
      "3\tof space\n",
      "\n",
      "Data:\n",
      "\n",
      "20 years ago, the compton gamma ray observatory plummeted through earth's atmosphere and splashed into the pacific ocean, ending its nearly decadelong quest to explore the highenergy cosmos.\n",
      "[xpost] we are the spacex software team, ask us anything\n",
      "\\\n",
      "a complete guide to saturn and its moons, everything cassini found out about the system and why nasa is launching dragonfly in 2026 to explore titan\n",
      "a ham radio enthusiast from ahmedabad india on wednesday got a response from spacex crew dragon's astronauts while trying to connect with international space station.\n",
      "a super cool timelapse of spacex assembling the falcon heavy\n",
      "astronomers caught sight of a black hole blowing bubbles into space. each bubble contained about 400 million billion pounds of matter \\u2014 about 1,000 halley\\u2019s comets worth.\n",
      "astronomers may have found an earthlike exoplanet orbiting a sunlike star\n",
      "astrophysics ask me anything  i'm astroparticle physics professor aaron vincent, i will be on facebook live at 1100 am edt and taking questions on reddit after 1200 pm edt. (more info in comments)\n",
      "chance of finding young earthlike planets higher than previously thought\n",
      "china's space station ban\n",
      "china's space station ban (i had no idea)\n",
      "explore space for yourself\n",
      "hey everyone, my name is temidayo oniosun the founder and managing director of space in africa, the authority on news, data and market analysis for the african space industry. ama\n",
      "i made a video on the story of space\n",
      "i stitched my own astrophotography together to show people just how deep you can explore into our wilkyway galaxy as an amateur astronomer.\n",
      "i\\u2019m looking for a good podcast about space. any suggestions?\n",
      "is there footage from inside the iss airlock of astronauts exiting the airlock for eva?\n",
      "list of everyplace humans have landed or crashed robots on mars\n",
      "nasa awards northrop grumman artemis contract for gateway crew cabin\n",
      "nasa denies the earlier reported ham radio communication\n",
      "nasa is considering a roverdeployed telescope on the moon that would consist of 128 radio antennas arranged in a flowerlike shape about 6 miles wide. they're also considering building massive radio telescopes inside of lunar craters, much like arecibo observatory (from contact, goldeneye, etc).\n",
      "potus campaign removes video that broke nasa ad rules\n",
      "spacex nearing 500 starlink satellites now in orbit after eighth successful launch\n",
      "spying a rare 'ring of fire' around venus at inferior conjunction\n",
      "supercluster explains why rockets go up\n",
      "the 'mole' on mars is finally underground after a push from nasa's insight lander\n",
      "the future of space | solr edits\n",
      "week of may 31, 2020 'all space questions' thread\n",
      "what if the reason we haven't found alien signals is because they don't have the technology to send them out?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data = curlResponse\n",
    "combinedngram = ''\n",
    "\n",
    "getdataSizeKB = lambda inputdata : str(str(round(sys.getsizeof(inputdata) / 1024, 2)) + \"\\tKB\")\n",
    "\n",
    "if killCase == 1:\n",
    "    data = data.lower()\n",
    "\n",
    "if parse == 1:\n",
    "    #print(data)\n",
    "    print(\"Matching regex: \" + str(parseRegex.pattern))\n",
    "    print(\"Before size:\\t\" + getdataSizeKB(data))\n",
    "    parsedfindall = sorted(set(re.findall(parseRegex, data))) #sort/uniq to remove repeated found patterns of same text\n",
    "    parsedData = ''\n",
    "    for item in parsedfindall:\n",
    "        parsedData = parsedData + str(item) + \"\\n\"\n",
    "    data = parsedData\n",
    "    print(\"After size:\\t\" + getdataSizeKB(data))\n",
    "\n",
    "if replacebadRegex == 1:\n",
    "    print(\"\\nRemoving data matching regex: \" + str(badRegex.pattern))\n",
    "    print(\"Before size:\\t\" + getdataSizeKB(data))\n",
    "    data = re.sub(badRegex, ' ', data) \n",
    "    print(\"After size:\\t\" + getdataSizeKB(data))\n",
    "\n",
    "if replacebadChars == 1:\n",
    "    print(\"\\nRemoving the following chars: \" + str(badChars))\n",
    "    print(\"Before size:\\t\" + getdataSizeKB(data))\n",
    "    for i in badChars : \n",
    "        data = data.replace(i, '') \n",
    "    print(\"After size:\\t\" + getdataSizeKB(data))\n",
    "\n",
    "if onlybasicChars == 1:\n",
    "    print(\"\\nRemoving non-alphanumeric except spaces: \" + str(alphanumspaceRegex.pattern))\n",
    "    print(\"Before size:\\t\" + getdataSizeKB(data))\n",
    "    \"\".join(i for i in data if ord(i)<128)\n",
    "    data = re.sub(alphanumspaceRegex, '', data)\n",
    "    print(\"After size:\\t\" + getdataSizeKB(data))\n",
    "\n",
    "#Single character splitting vs default of space\n",
    "if splitbychar == 1:\n",
    "    print(\"Splitting by character\")\n",
    "    def split(word): \n",
    "        return [char for char in word]  \n",
    "    tokenized = split(data)\n",
    "else: \n",
    "    tokenized = data.split()\n",
    "    #print(tokenized)\n",
    "    \n",
    "ngramstart = time.time()\n",
    "#iterate ngram length from ngramMin to ngramMax\n",
    "for i in range(ngramMax,(ngramMin - 1),-1):\n",
    "    #print('ngram length:' + str(i) + '\\n')\n",
    "    ngramResults = ngrams(tokenized, i)\n",
    "    ngramFreq = collections.Counter(ngramResults)\n",
    "    ngramCommon = ngramFreq.most_common(mostcommonMax)\n",
    "    #print(ngramCommon)\n",
    "    #Split results for text comparison\n",
    "    for list in ngramCommon:\n",
    "        ngramStr = ' '.join(str(character) for character in list[0])\n",
    "        ngramCount = list[1]\n",
    "        if ngramRedundant == 1:\n",
    "            if ngramCount > minngramCount:\n",
    "                combinedngram = combinedngram + str(ngramCount) + \"\\t\" + ngramStr + \"\\n\"\n",
    "        else:\n",
    "            #Only add if not found in larger match\n",
    "            if combinedngram.find(ngramStr) == -1 and ngramCount > minngramCount:\n",
    "                combinedngram = combinedngram + str(ngramCount) + \"\\t\" + ngramStr + \"\\n\"\n",
    "            \n",
    "\n",
    "print(\"\\n\" + (\"-\" * 75))\n",
    "print(\"\\nCommon ngrams, longest to shortest: \" + \"\\n\\n\" + combinedngram)\n",
    "print(\"-\" * 75)\n",
    "print(\"ngram search time:\\t\", time.time() - ngramstart, \"seconds\")\n",
    "print(\"Total processing time:\\t\", time.time() - start, \"seconds\")\n",
    "print(\"\\nSource:\\t\" + dataURL)\n",
    "print(\"-\" * 75)\n",
    "\n",
    "#Show response if enabled\n",
    "if showresponses == 1:\n",
    "    print(\"\\n\\nshowresponses = 1, displaying data...\")\n",
    "    curlResponseSize = sys.getsizeof(curlResponse)\n",
    "    dataSize = sys.getsizeof(data)\n",
    "    combinedngramSize = sys.getsizeof(combinedngram)\n",
    "    #print(combinedngramSize,dataSize,curlResponseSize)\n",
    "    #Don't print huge amounts of data\n",
    "    if (combinedngramSize < responsesizeMax):\n",
    "        print(\"combinedngram:\\n\\n\" + combinedngram)\n",
    "    else:\n",
    "        print(\"combinedngram larger than max response size in config - \" + str(combinedngramSize) + \" bytes, max is \" + str(responsesizeMax))\n",
    "    if (dataSize < responsesizeMax):\n",
    "        print(\"Data:\\n\\n\" + data)\n",
    "    else:\n",
    "        print(\"data larger than max response size in config - \" + str(dataSize) + \" bytes, max is \" + str(responsesizeMax))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
